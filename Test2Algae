#reference: https://www.kaggle.com/code/rafaymemon/algae-growth-in-artificial-water-bodies/notebook



import pandas as pd
import numpy as np
import seaborn as sns 
import matplotlib.pyplot as plt 

##
import pandas as pd
file_path = "C:/Users/Farahida Hanim/OneDrive - Universiti Teknologi PETRONAS/algeas.csv"
df = pd.read_csv(file_path)
#df

#Visualization-------------------------
# To check null values in dataset
#null_total = df.isnull().sum()
#print(null_total) 


#to show the mean, median, mode, max, min, std, etc-----------------------------
#print(df.describe())  

#scatterplot---------------------------------------------------------
#sns.pairplot(df)
#plt.show() 
#light vs CO2 
#sns.scatterplot(x='Light',y='CO2',data=df)
#plt.show()

#boxplot
#/for feature in df.columns:
    # plt.figure(figsize=(6, 4))
    # sns.boxplot(x=df[feature], orient='vertical', palette='Set2')
    # plt.title(f'Boxplot of {feature}')
    # plt.xlabel('Values')
    # plt.ylabel('')
    # plt.xticks(rotation=0)
    # plt.tight_layout()
    # plt.show() 
#---------------------------------------------
#histogram--------------------------------------------------------- 

# for feature in df.columns:
#     plt.figure(figsize=(6, 4))
#     sns.histplot(df[feature], color='skyblue', bins=10, kde=True)
#     plt.title(f'Histogram of {feature}')
#     plt.xlabel('Values')
#     plt.ylabel('Frequency')
#     plt.xticks(rotation=0)
#     plt.tight_layout()
#     plt.show()

#Model Building--------------------------------------------------------- 

#Regression  
#Let's create numpy arrays for features and target
X = df.drop('Population',axis=1).values
y = df['Population'].values
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_scaled = scaler.fit_transform(X_train)
X_scaled_test = scaler.fit_transform(X_test) 
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split

# Sample data for illustration purposes (replace with your dataset)
# Assume 'X' contains your features and 'y' contains your target variable
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define a dictionary of regression algorithms
regressors = {
    'Linear Regression': LinearRegression(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'Random Forest Regressor': RandomForestRegressor(),
    'Support Vector Regressor': SVR(),
    'K-Nearest Neighbors Regressor': KNeighborsRegressor()
}

# Dictionary to store results
results = {}

# Loop through each regression algorithm
for name, reg in regressors.items():
    reg.fit(X_scaled, y_train)
    y_pred = reg.predict(X_scaled_test)

    # Calculate regression metrics
    mse = mean_squared_error(y_test, y_pred)
    #rmse = mean_squared_error(y_test, y_pred, squared=False)
    mae = mean_absolute_error(y_test, y_pred)
    r_squared = r2_score(y_test, y_pred)

    results[name] = {
        'Mean Squared Error (MSE)': mse,
       # 'Root Mean Squared Error (RMSE)': rmse,
        'Mean Absolute Error (MAE)': mae,
        'R-squared (R2)': r_squared
    }

# Print results for each regression algorithm------------------------------------
# for name, metrics in results.items():
#     print(f"Regressor: {name}")
#     print(f"Mean Squared Error (MSE): {metrics['Mean Squared Error (MSE)']:.2f}")
#    # print(f"Root Mean Squared Error (RMSE): {metrics['Root Mean Squared Error (RMSE)']:.2f}")
#     print(f"Mean Absolute Error (MAE): {metrics['Mean Absolute Error (MAE)']:.2f}")
#     print(f"R-squared (R2): {metrics['R-squared (R2)']:.2f}")
#     print("=" * 40) 

#------------------------------------------
#from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
#import numpy as np

# Define the hyperparameter grid to search
param_grid = {
    'n_estimators': np.arange(50, 201, 10),  # Number of trees in the forest
    'max_depth': np.arange(10, 101, 10),    # Maximum depth of each tree
    'min_samples_split': [2, 5, 10],       # Minimum samples required to split an internal node
    'min_samples_leaf': [1, 2, 4],         # Minimum samples required to be a leaf node
    'bootstrap': [True, False]             # Whether bootstrap samples are used
}

# Create a Random Forest Regressor
rf_regressor = RandomForestRegressor()

# Create RandomizedSearchCV object
random_search = RandomizedSearchCV(estimator=rf_regressor, param_distributions=param_grid, n_iter=100,
                                   scoring='neg_mean_squared_error', cv=5, random_state=42, n_jobs=-1)

# Fit the RandomizedSearchCV to your data
random_search.fit(X_scaled, y_train)

# Print the best hyperparameters found
print("Best Hyperparameters:", random_search.best_params_)

# Get the best model from the search
best_rf_model = random_search.best_estimator_



# Print the best model
print(best_rf_model)

# Evaluate the best model on your test set
y_pred = best_rf_model.predict(X_scaled_test)


# # Calculate evaluation metrics (e.g., MSE, RMSE, MAE, R2) on the test set
# mse = mean_squared_error(y_test, y_pred)
# mae = mean_absolute_error(y_test, y_pred)
# r_squared = r2_score(y_test, y_pred) 

# print(f"Mean Squared Error (MSE): {mse:.2f}")
# print(f"Mean Absolute Error (MAE): {mae:.2f}")
# print(f"R-squared (R2): {r_squared:.2f}")
# Evaluate the model's performance on your specific regression task